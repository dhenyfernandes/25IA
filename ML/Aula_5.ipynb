{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fourth-relaxation",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-owner",
   "metadata": {},
   "source": [
    "### Motivação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(4, random_state=0)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # plot the input data\n",
    "    plt.figure(figsize=(8, 8), dpi=80)\n",
    "    ax = ax or plt.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(kmeans, X_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-fairy",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vetor de pesos (fi)\n",
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-parts",
   "metadata": {},
   "source": [
    "Voltando aos dados do exemplo de motivação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gmm.predict_proba(X)\n",
    "print(probs[:5].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
    "plot_gmm(gmm, X_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-chosen",
   "metadata": {},
   "source": [
    "### Fazendo predição de novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = gm.sample(6)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-career",
   "metadata": {},
   "source": [
    "### Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n",
    "plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_ds = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\n",
    "plot_gmm(gmm_ds, Xmoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)\n",
    "plot_gmm(gmm16, Xmoon, label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = gmm16.sample(400)\n",
    "plt.scatter(Xnew[0][:,0], Xnew[0][:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-quarterly",
   "metadata": {},
   "source": [
    "### Tipo de Covariância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#função auxiliar definida na aula do kmeans para visualizar os centróides\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função auxiliar para plotar as fronteiras de decisão de uma GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                 levels=np.logspace(0, 2, 12))\n",
    "    plt.contour(xx, yy, Z,\n",
    "                norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                levels=np.logspace(0, 2, 12),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z,\n",
    "                linewidths=2, colors='r', linestyles='dashed')\n",
    "    \n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "    plot_centroids(clusterer.means_, clusterer.weights_)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#função auxiliar para comparar, 2 a 2, os diferentes tipos de covariância\n",
    "def compare_gaussian_mixtures(gm1, gm2, X):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_gaussian_mixture(gm1, X)\n",
    "    plt.title('covariance_type=\"{}\"'.format(gm1.covariance_type), fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_gaussian_mixture(gm2, X, show_ylabels=False)\n",
    "    plt.title('covariance_type=\"{}\"'.format(gm2.covariance_type), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando os 4 tipos de covariância\n",
    "gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type=\"full\", random_state=42)\n",
    "gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type=\"tied\", random_state=42)\n",
    "gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type=\"spherical\", random_state=42)\n",
    "gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type=\"diag\", random_state=42)\n",
    "gm_full.fit(X)\n",
    "gm_tied.fit(X)\n",
    "gm_spherical.fit(X)\n",
    "gm_diag.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparando tied e spherical\n",
    "compare_gaussian_mixtures(gm_tied, gm_spherical, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gaussian_mixtures(gm_full, gm_diag, X)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-extraction",
   "metadata": {},
   "source": [
    "## Detecção de Anomalia usando GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plot_gaussian_mixture(gm, X)\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], color='y', marker='*')\n",
    "plt.ylim(top=5.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-allocation",
   "metadata": {},
   "source": [
    "## Selecionando o número de Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.bic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-suspect",
   "metadata": {},
   "source": [
    "Vamos treinar vários GMM com vários valores de k e mensurar BIC e AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n",
    "             for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = [model.bic(X) for model in gms_per_k]\n",
    "aics = [model.aic(X) for model in gms_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora plotamos os valores para verificar o melhor valor de k\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\n",
    "plt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Information Criterion\", fontsize=14)\n",
    "plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])\n",
    "plt.annotate('Minimum',\n",
    "             xy=(3, bics[2]),\n",
    "             xytext=(0.35, 0.6),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=14,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-journey",
   "metadata": {},
   "source": [
    "Por último, podemos procurar pela melhor combinação de valores tanto para o número de clusters quanto o tipo de covariância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bic = np.infty\n",
    "\n",
    "for k in range(1, 11):\n",
    "    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n",
    "        bic = GaussianMixture(n_components=k, n_init=10,\n",
    "                              covariance_type=covariance_type,\n",
    "                              random_state=42).fit(X).bic(X)\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            best_k = k\n",
    "            best_covariance_type = covariance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_covariance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-metro",
   "metadata": {},
   "source": [
    "# Topic Modelling usando Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-buddy",
   "metadata": {},
   "source": [
    "Algumas considerações antes de estudarmos LDA:\n",
    "\n",
    "* Como a disciplina não é de NLP (Processamento de Linguagem Natural), alguns métodos que utilizei aqui não serão explicados em detalhes, mas tentarei ser o mais didádito possível para torná-los compreensíveis\n",
    "\n",
    "* Gensim é uma biblioteca voltada para NLP que possui vários métodos de tratamento de textos e algoritmos para transformar textos\n",
    "\n",
    "* NLTK é um toolkit que contém vários métodos de tratamento de textos e dicionários incorporados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bases/abcnews-date-text.csv', error_bad_lines=False, nrows=500000)\n",
    "print(data.head())\n",
    "documents = data[['headline_text']]\n",
    "documents['index'] = documents.index\n",
    "print()\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-fossil",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados\n",
    "\n",
    "É uma etapa crucial na análise de textos, em que tratamos eles de maneira correta para usa-los como input para algoritmos de machine learning. Aqui, vamos executar os seguintes passos:\n",
    "\n",
    "1. Tokenização: divide o texto em sentenças e as sentenças em palavras; remove toda pontuação e deixa todas as palavras em minúscula\n",
    "\n",
    "2. Elimina palavra que possuem menos de 3 caracteres\n",
    "\n",
    "3. Elimina as stopwords\n",
    "\n",
    "4. Lematização e Stemização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-multimedia",
   "metadata": {},
   "source": [
    "A lematização é o processo de tornar uma palavra em sua raiz, conforme exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))\n",
    "# pos = part of speech -> tageamento de palavras conforme sua classe gramatical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-fountain",
   "metadata": {},
   "source": [
    "Stemização é o processo de eliminar os radicais de uma palavra, conforme exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-trademark",
   "metadata": {},
   "source": [
    "Vamos escrever duas funções:\n",
    "\n",
    "1. Aplicação de lemmatização e stemização nas palavras\n",
    "\n",
    "2. pré-processamento do texto conforme indicado acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-control",
   "metadata": {},
   "source": [
    "Vamos pegar um exemplo e comparar sua forma original com sua forma tratada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = documents[documents['index'] == 620].values[0][0]\n",
    "\n",
    "print('Documento original: ')\n",
    "print(documents[documents['index'] == 620])\n",
    "print()\n",
    "print('Palavras do Documento original')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n Documento processado: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-montgomery",
   "metadata": {},
   "source": [
    "Vamos aplicar o pré-processamento a todo o texto agora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-orleans",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "* A partir de processed_docs, vamos criar um dicionário que contém o número de vezes que uma palavra apareceu no conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-component",
   "metadata": {},
   "source": [
    "Agora, vamos aplicar um filtros nos tokens que aparecem em:\n",
    "\n",
    "1. menos que 15 números;\n",
    "\n",
    "2. mais que 0.5 documentos (fração do corpus total (total de documentos))\n",
    "\n",
    "3. depois dos dois primeiros passos, mantém apenas os primeiros 100000 tokens mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-former",
   "metadata": {},
   "source": [
    "### doc2bow\n",
    "\n",
    "Para cada documento, criamos um dicionário que relata palavras e quantas vezes elas apareceram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[620]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-paragraph",
   "metadata": {},
   "source": [
    "Vamos olhar como fica nosso exemplo pre-processado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_620 = bow_corpus[620]\n",
    "\n",
    "for i in range(len(bow_doc_620)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_620[i][0], \n",
    "                                                     dictionary[bow_doc_620[i][0]], \n",
    "                                                     bow_doc_620[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-newton",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "* TF (Term-Frequency): quanto mais uma palavra aparece num documento, mais relevante ela é para descrever esse documento\n",
    "\n",
    "* IDF (Inverse-Document Frequency): palavras que aparecem raras vezes num documento também podem ser bons descritores desse documento\n",
    "\n",
    "A ideia do TF-IDF é ponderar a relevância das palavras num documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-coast",
   "metadata": {},
   "source": [
    "## Executando LDA usando Bag-of-Words\n",
    "\n",
    "Após realizar o tratamento dos textos, vamos agora executar o LDA e analisar seus resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-shirt",
   "metadata": {},
   "source": [
    "Para cada tópilo, vamos explorar a ocorrência de palavras neste tópico e seu peso relativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-prime",
   "metadata": {},
   "source": [
    "### Executando LDA usando TF-IDF\n",
    "Vamos executar o LDA usando a outra abordagem agora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-verification",
   "metadata": {},
   "source": [
    "## Avaliação dos Modelos\n",
    "Vamos agora avaliar os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-wales",
   "metadata": {},
   "source": [
    "### Usando Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[620]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-asian",
   "metadata": {},
   "source": [
    "### Usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[620]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-standing",
   "metadata": {},
   "source": [
    "## Testando o modelo em documentos não vistos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-links",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
